function [mu,C,w]=gaussianmixture(x,k,verbosity) ;
% GAUSSIANMIXTURE Calculate Gaussian mixture parameters
% CMP Vision Algorithms http://visionbook.felk.cvut.cz
% 
% A Gaussian mixture model  is described by means mu_k,
% covariances Sigma_k and weights w_k. 
%  The parameters mu_k, Sigma_k, and
% w_k can be estimated from N samples x_1,...,x_N 
% using the expectation-maximization (EM) algorithm
% . The algorithm performs well in practice even though
% global optimality of the result is not guaranteed.
%
% Usage: [mu,C,w] = gaussianmixture(x,k,verbosity)
% Inputs:
%   x  [d x N]  Input data matrix. Each column is one
%     d-dimensional sample (point).
%   k  [1 x 1]  The number K of Gaussians to use.
%   verbosity  (default 0)  If set to 1, progress is reported. 
%   The corresponding code is omitted here.
% Outputs:
%   mu  [d x K]  Centers mu_k.
%   C   [d x d x K]  Covariances Sigma_k.
%   w   [K x 1]  Weights w_k.
%
% A suitable number of Gaussians K can be found either experimentally
% or by using more sophisticated approaches, such as the minimum description
% length principle .
% 

if nargin<3,
  verbosity=0 ;
end ;
  
% The initial centers mu_k are found using kmeans clustering.
% The transpose (x', mu') serves to harmonize row/column
% conventions which are unfortunately not consistently used in Matlab.

[d,n] = size(x);
[idx,mu] = kmeans( x', k );
mu = mu';

% The initial covariances C of each cluster (Gaussian) are
% calculated. Note that C is a 3D matrix (table). The weights
% w are initialized as uniform.

C = zeros( d, d, k );

for i = 1:k
  C(:,:,i) = cov( x(:,idx==i)' );
end

w = ones(n,1)/k;

% The expectation-maximization (EM) core consists of a loop which is normally
% exited through the break statement (below) when convergence is
% detected. In the expectation step, we calculate for all data points 
% x_i and all Gaussian components the probability that
% a particular point is generated by a particular component. 
% (The function gausspdf is given below.) This
% probability is stored in p and normalized to sum to one for
% each point. The new weights w are the means of p for
% each Gaussian over all points.

for iter = 1:10000
  p = zeros( k, n );
  for i = 1:k
    p(i,:) = w(i) * gausspdf( x, mu(:,i), C(:,:,i) );
  end
  p = p ./ repmat(sum(p),k,1);
  w = mean( p, 2 );

% In the maximization step, the new parameters mu_k and Sigma_k are
% calculated using sample means and covariances weighted by the
% probabilities p . sump serves to
% normalize p across components.

  oldmu = mu;  oldC = C;
  sump = sum( p, 2 );
  for i = 1:k
    mu(:,i) = x*p(i,:)'/sump(i);
    dif = x - repmat( mu(:,i), 1, n );
    C(:,:,i) = ( repmat(p(i,:),d,1).*dif ) * dif'/sump(i);
  end

% Convergence is detected by comparing the maximum change of the
% Gaussian parameters mu_k and Sigma_k with a threshold. 
% This normally works well,
% however, for better flexibility,
% relative error could be tested too and both thresholds
% could be made user-selectable.

  e = max( [abs(mu(:)-oldmu(:))' abs(C(:)-oldC(:))'] );
  
  if verbosity>0
    disp(['Iteration: ' num2str(iter) ' change=' num2str(e) ]) ;
  end ;
  

  if e<1e-6, break; end  
end % for iter

% 
% Usage: prob = gausspdf(x,mean,sigma)
%
% Given a mean mean (as a column vector) 
% and covariance matrix sigma
% of a d-dimensional Gaussian distribution, evaluate the probability
% density function at all points x. Each column of x
% corresponds to one point.

function prob=gausspdf(x,mean,sigma) ;
[d,n] = size(x);
prb = zeros( n, 1 );
sigmainv = inv(sigma);
c = (2*pi)^(-0.5*d) * sqrt( det(sigmainv) );
for i = 1:n
  dif = x(:,i)-mean;
  prob(i) = c * exp( -0.5*dif'*sigmainv*dif );
end

